# -*- coding: utf-8 -*-
"""Bank Churner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TH89Ejb1ha_HA0TaCU5yF5-4V_lYVrFf

# Library
"""

'''
Library yang digunakan untuk proyek ini
'''
import pandas as pd
import numpy as np
import seaborn as sns
import scipy.stats as scp
import plotly.express as px
import matplotlib.pyplot as plt
from numpy.polynomial.polynomial import polyfit
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from sklearn import cluster
from sklearn.metrics import pairwise_distances
from scipy.cluster import hierarchy
import scipy.stats as scp

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor
from sklearn.feature_selection import SelectKBest
from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, LogisticRegression, Perceptron, Ridge, SGDClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, pairwise_distances, r2_score, silhouette_samples, silhouette_score
from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from xgboost import XGBClassifier
from collections import Counter
from sklearn.decomposition import PCA
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN
from imblearn.under_sampling import NearMiss

pd.set_option('display.max_rows',None)

"""# Bank Churner

## Domain proyek

### `Analisis Churn (Pergantian Nasabah) dalam Industri Perbankan`


---


**Latar Belakang:**

Industri perbankan menghadapi tantangan yang signifikan dalam mempertahankan nasabah yang ada. Pergantian nasabah atau churn dapat menyebabkan kerugian keuangan dan reputasi bagi lembaga keuangan. Oleh karena itu, penting untuk memahami faktor-faktor yang mempengaruhi churn nasabah dan mengembangkan strategi yang efektif untuk meminimalkan churn.

**Penelitian terkait:**

Smith, J., & Johnson, A. (Tahun). "Customer Churn Prediction in Retail Banking: A Data Mining Approach." Journal of Business Research.

Brown, L., & Jones, B. (Tahun). "Predicting Customer Churn in Retail Banking: A Comparative Study of Machine Learning Techniques." Expert Systems with Applications.

## Business Understanding

**Pernyataan Masalah**

*   Pernyataan Masalah 1:
Berdasarkan latar belakang yang telah disajikan, masalah yang ingin diselesaikan adalah bagaimana memprediksi churn nasabah dalam industri perbankan untuk mengambil langkah-langkah pencegahan yang diperlukan.

*   Pernyataan Masalah 2:
Masalah lain yang ingin diatasi adalah bagaimana mengidentifikasi faktor-faktor utama yang berkontribusi terhadap churn nasabah dalam industri perbankan.

**Goals**

*   Jawaban Pernyataan Masalah 1:

Tujuan dari pernyataan masalah pertama adalah mengembangkan model prediksi churn nasabah yang akurat berdasarkan faktor-faktor yang relevan dalam industri perbankan. Hal ini akan membantu lembaga keuangan mengidentifikasi nasabah yang berpotensi churn dan mengambil tindakan yang sesuai untuk mempertahankan mereka.

*   Jawaban Pernyataan Masalah 2:

Tujuan dari pernyataan masalah kedua adalah mengidentifikasi faktor-faktor kunci yang berpengaruh terhadap churn nasabah dalam industri perbankan. Dengan pemahaman yang lebih baik tentang faktor-faktor ini, lembaga keuangan dapat mengambil langkah-langkah pencegahan yang lebih efektif dan mempertahankan nasabah mereka.

**Solution statements**

*   Menggunakan teknik machine learning seperti regresi logistik, pohon keputusan, algoritma naive bayes, dan algoritma lainnya untuk membangun model prediksi churn nasabah.

*   Melakukan analisis fitur untuk menentukan faktor-faktor yang paling penting dalam mempengaruhi churn nasabah dan menggunakan informasi ini untuk meningkatkan keputusan bisnis.

*   Solusi yang diusulkan akan dievaluasi menggunakan metrik evaluasi seperti akurasi, presisi, recall, dan f1-score untuk memastikan keandalan dan kinerja model prediksi churn nasabah.

*   Melakukan hyperparameter tuning dengan GridSearchCV pada model terbaik.

## Data Understanding
"""

# Import dataset Bank Churner
df_bank = pd.read_csv(
    'https://drive.google.com/u/2/uc?id=1-4tna2lDs6Dchd6UyXQgoCA738j1z2Bh')
df_bank.tail()

df_bank.shape

df_bank.info()

"""**Variabel-variabel pada dataset adalah sebagai berikut:**
1. CLIENTNUM: Nomor identifikasi unik untuk setiap nasabah dalam dataset. Ini adalah variabel yang digunakan untuk mengidentifikasi individu secara unik.

2. Attrition_Flag: Variabel ini menunjukkan apakah nasabah churned (pergi) atau masih aktif. Nilai "Existing Customer" menunjukkan nasabah yang masih aktif, sementara nilai "Attrited Customer" menunjukkan nasabah yang telah churned.

3. Customer_Age: Variabel ini menunjukkan usia nasabah dalam tahun.

4. Gender: Variabel ini menunjukkan jenis kelamin nasabah. Nilai "M" menunjukkan laki-laki, sementara nilai "F" menunjukkan perempuan.

5. Dependent_count: Variabel ini menunjukkan jumlah orang yang menjadi tanggungan nasabah, seperti pasangan atau anak-anak.

6. Education_Level: Variabel ini menunjukkan tingkat pendidikan nasabah. Nilai-nilai yang mungkin termasuk "Unknown", "Uneducated", "High School", "College", "Graduate", "Post-Graduate", dan "Doctorate".

7. Marital_Status: Variabel ini menunjukkan status perkawinan nasabah. Nilai-nilai yang mungkin termasuk "Unknown", "Single", "Married", "Divorced", dan "Separated".

8. Income_Category: Variabel ini menunjukkan kategori pendapatan nasabah. Nilai-nilai yang mungkin termasuk "Unknown", "Less than $40K", "$40K - $60K", "$60K - $80K", "$80K - $120K", dan "$120K +".

9. Card_Category: Variabel ini menunjukkan jenis kategori kartu kredit yang dimiliki oleh nasabah. Nilai-nilai yang mungkin termasuk "Blue", "Silver", "Gold", dan "Platinum".

10. Months_on_book: Variabel ini menunjukkan jumlah bulan sejak nasabah pertama kali membuka rekening.

11. Total_Relationship_Count: Variabel ini menunjukkan jumlah produk keuangan yang dimiliki oleh nasabah di bank tersebut.

12. Months_Inactive_12_mon: Variabel ini menunjukkan jumlah bulan ketika nasabah tidak aktif dalam 12 bulan terakhir.

13. Contacts_Count_12_mon: Variabel ini menunjukkan jumlah kali nasabah dihubungi oleh bank dalam 12 bulan terakhir.

14. Credit_Limit: Variabel ini menunjukkan batas kredit yang diberikan kepada nasabah.

15. Total_Revolving_Bal: Variabel ini menunjukkan saldo total dari kredit yang belum dibayar oleh nasabah.

16. Avg_Open_To_Buy: Variabel ini menunjukkan rata-rata jumlah uang yang tersedia untuk nasabah dalam melakukan pembelian dengan kartu kredit.

17. Total_Amt_Chng_Q4_Q1: Variabel ini menunjukkan persentase perubahan total jumlah transaksi nasabah dari kuartal keempat ke kuartal pertama.

18. Total_Trans_Amt: Variabel ini menunjukkan jumlah total transaksi yang dilakukan oleh nasabah.

19. Total_Trans_Ct: Variabel ini menunjukkan jumlah total transaksi yang dilakukan oleh nasabah.

20. Total_Ct_Chng_Q4_Q1: Variabel ini menunjukkan persentase perubahan total jumlah transaksi nasabah dari kuartal keempat ke kuartal pertama.

21. Avg_Utilization_Ratio: Variabel ini menunjukkan rasio penggunaan rata-rata dari total kredit yang tersedia kepada nasabah.

#### Understading Missing Value Data
"""

# Fungsi untuk melihat jumlah missing values
def cek_null(df):
    col_na = df.isnull().sum().sort_values(ascending=True)
    percent = col_na / len(df)

    missing_data = pd.concat([col_na, percent], axis=1, keys=['Total', 'Percent'])

    if (missing_data[missing_data['Total'] > 0].shape[0] == 0):
        print("Tidak ditemukan missing value pada dataset")

    else:
        print(missing_data[missing_data['Total'] > 0])

cek_null(df_bank)

"""#### Understading Duplicate Data"""

df_bank.duplicated().sum()

"""#### Understading Outliers Data"""

# Melihat jumlah outlier

Jumlah_Outlier = []

for i in df_bank.columns:

  # Melakukan looping untuk mencari outlier setiap kolom yang isinya bukan tipe data object
  if (df_bank[i].dtypes != 'object'):
    data = df_bank[i]

    # Mencari nilai IQR
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1

    # Mencari RUB dan RLB
    RLB = Q1 - 1.5 * IQR
    RUB = Q3 + 1.5 * IQR

    # Mencari banyak outlier
    outlier_lower = data[data < RLB]
    outlier_upper = data[data > RUB]
    jumlah_outlier = len(outlier_lower) + len(outlier_upper)
    persentase_outlier = jumlah_outlier/len(df_bank)

    Jumlah_Outlier.append([i, jumlah_outlier, persentase_outlier])

pd.DataFrame(Jumlah_Outlier, columns=['Column', 'Jumlah Outliers','Persentase Outliers']).sort_values(by=['Jumlah Outliers'], ascending = False,ignore_index=True)

"""### EDA"""

df_eda = df_bank.copy()
df_eda.tail()

"""#### **Apa tingkat pendidikan pelanggan dengan rata-rata total transaksi terbesar?**"""

df_mean = df_eda.groupby('Education_Level').mean().reset_index()
df_mean.head()

data = df_mean.sort_values(by='Total_Trans_Ct')
fig = plt.figure(figsize = (30, 15))
bars = plt.bar(data['Education_Level'], data['Total_Trans_Ct'])

for i, bar in enumerate(bars):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(round(data['Total_Trans_Ct'][i], 2)), ha='center', va='bottom')

plt.xlabel("Education_Level")
plt.ylabel('Total_Trans_Ct')
plt.title('Education_Level On Mean Total_Trans_Ct')
plt.show()

"""#### **Bagaimana karakteristik pelanggan yang telah menjadi "Attrited Customer"?**"""

df_attrited  = df_eda.loc[df_eda['Attrition_Flag'] == 'Attrited Customer']
df_attrited.tail()

df_attrited.info()

df_attrited.describe()

df_attrited.describe(include=['object'])

jumlah_nasabah = []

for i in df_eda['Card_Category'].unique():
  total = len(df_eda.loc[df_eda['Card_Category'] == i])
  total_cond = len(df_attrited.loc[df_eda['Card_Category'] == i])

  if total != 0:
    jumlah_nasabah.append([i, total_cond, total, total_cond/total])
  else:
    jumlah_nasabah.append([i, total_cond, total, 0])

pd.DataFrame(jumlah_nasabah, columns=['Card_Category', 'Total Nasabah dengan Kondisi', 'Total Nasabah', 'Persentase Nasabah dengan Kondisi']).sort_values(by=['Total Nasabah dengan Kondisi'], ascending = False,ignore_index=True)

jumlah_nasabah = []

for i in df_eda['Education_Level'].unique():
  total = len(df_eda.loc[df_eda['Education_Level'] == i])
  total_cond = len(df_attrited.loc[df_eda['Education_Level'] == i])

  if total != 0:
    jumlah_nasabah.append([i, total_cond, total, total_cond/total])
  else:
    jumlah_nasabah.append([i, total_cond, total, 0])

pd.DataFrame(jumlah_nasabah, columns=['Education_Level', 'Total Nasabah dengan Kondisi', 'Total Nasabah', 'Persentase Nasabah dengan Kondisi']).sort_values(by=['Total Nasabah dengan Kondisi'], ascending = False,ignore_index=True)

jumlah_nasabah = []

for i in df_eda['Income_Category'].unique():
  total = len(df_eda.loc[df_eda['Income_Category'] == i])
  total_cond = len(df_attrited.loc[df_eda['Income_Category'] == i])

  if total != 0:
    jumlah_nasabah.append([i, total_cond, total, total_cond/total])
  else:
    jumlah_nasabah.append([i, total_cond, total, 0])

pd.DataFrame(jumlah_nasabah, columns=['Income_Category', 'Total Nasabah dengan Kondisi', 'Total Nasabah', 'Persentase Nasabah dengan Kondisi']).sort_values(by=['Total Nasabah dengan Kondisi'], ascending = False,ignore_index=True)

"""#### **Apakah terdapat hubungan antara jenis kartu yang dimiliki nasabah dengan pendapatannya?**

##### Income_Category

Less than $40K
"""

df_40 = df_eda.loc[df_eda['Income_Category'] == 'Less than $40K'].reset_index()
df_40.tail()

data = df_40['Card_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Card_Category On Income_Category Less than $40K')
plt.show()

"""$40K - $60K"""

df_40_60 = df_eda.loc[df_eda['Income_Category'] == '$40K - $60K'].reset_index()
df_40_60.tail()

data = df_40_60['Card_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Card_Category On Income_Category $40K - $60K')
plt.show()

"""$60K - $80K"""

df_60_80 = df_eda.loc[df_eda['Income_Category'] == '$60K - $80K'].reset_index()
df_60_80.tail()

data = df_60_80['Card_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Card_Category On Income_Category $60K - $80K')
plt.show()

"""$80K - $120K"""

df_80_120 = df_eda.loc[df_eda['Income_Category'] == '$80K - $120K'].reset_index()
df_80_120.tail()

data = df_80_120['Card_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Card_Category On Income_Category $80K - $120K')
plt.show()

"""$120K +"""

df_120 = df_eda.loc[df_eda['Income_Category'] == '$120K +'].reset_index()
df_120.tail()

data = df_120['Card_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Card_Category On Income_Category $120K +')
plt.show()

"""##### Card_Category

Blue
"""

df_blue = df_eda.loc[df_eda['Card_Category'] == 'Blue'].reset_index()
df_blue.tail()

data = df_blue['Income_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Income_Category On Card_Category Blue')
plt.show()

"""Silver"""

df_silver = df_eda.loc[df_eda['Card_Category'] == 'Silver'].reset_index()
df_silver.tail()

data = df_silver['Income_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Income_Category On Card_Category Silver')
plt.show()

"""Gold"""

df_gold = df_eda.loc[df_eda['Card_Category'] == 'Gold'].reset_index()
df_gold.tail()

data = df_gold['Income_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Income_Category On Card_Category Gold')
plt.show()

"""Platinum"""

df_platinum = df_eda.loc[df_eda['Card_Category'] == 'Platinum'].reset_index()
df_platinum.tail()

data = df_platinum['Income_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Income_Category On Card_Category Platinum')
plt.show()

"""$120K +"""

df_120 = df_eda.loc[df_eda['Income_Category'] == '$120K +'].reset_index()
df_120.tail()

data = df_120['Card_Category'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('Card_Category On Income_Category Less than $40K')
plt.show()

"""#### **Apakah nasabah yang tidak aktif selama kurang dari 3 bulan dalam kurun waktu 12 bulan memiliki saldo diatas rata-rata?**"""

df_4 = df_eda.copy()

mean = df_4['Total_Revolving_Bal'].mean()
mean

df_4['High_Total_Revolving_Bal'] = None
df_4['High_Total_Revolving_Bal'].loc[df_4['Total_Revolving_Bal'] > mean] = 'Yes'
df_4['High_Total_Revolving_Bal'].loc[df_4['Total_Revolving_Bal'] <= mean] = 'No'
df_4.tail()

df_inactive_less_3 = df_4.loc[df_4['Months_Inactive_12_mon'] < 3].reset_index()
df_inactive_less_3.tail()

df_inactive_more_3 = df_4.loc[df_4['Months_Inactive_12_mon'] >= 3].reset_index()
df_inactive_more_3.tail()

df_inactive_less_3['High_Total_Revolving_Bal'].value_counts().plot(kind='bar');

df_inactive_more_3['High_Total_Revolving_Bal'].value_counts().plot(kind='bar');

data = df_inactive_less_3['High_Total_Revolving_Bal'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('High_Total_Revolving_Bal On Months_Inactive_12_mon Less than 3 month')
plt.show()

data = df_inactive_more_3['High_Total_Revolving_Bal'].value_counts()
plt.figure(figsize=(20,20))
plt.pie(data.tolist(), labels = data.index.tolist(), autopct='%.1f%%')
plt.title('High_Total_Revolving_Bal On Months_Inactive_12_mon More than 3 month')
plt.show()

"""#### **Apakah jumlah tanggungan seorang nasabah dapat mempengaruhi limit dari kartu kredit nasabah tersebut?**"""

df_mean = df_eda.groupby('Dependent_count').mean().reset_index()
df_mean

data = df_mean.sort_values(by='Credit_Limit')
fig = plt.figure(figsize = (30, 15))
bars = plt.bar(data['Dependent_count'], data['Credit_Limit'])

for i, bar in enumerate(bars):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(round(data['Credit_Limit'][i], 2)), ha='center', va='bottom')

plt.xlabel("Dependent_count")
plt.ylabel('Credit_Limit')
plt.title('Dependent_count On Mean Credit_Limit')
plt.show()

"""## Data Preparation"""

df_bank = df_bank.drop(['CLIENTNUM'], axis=1)

def unknown(column):
  lst = []
  for i in df_bank[column]:
    if i == 'Unknown':
      i = None
    lst.append(i)
  df_bank[column] = lst

"""### Data Cleaning

#### Missing Value
"""

cek_null(df_bank)

df_bank = df_bank.fillna("Unknown")

cek_null(df_bank)

df_bank.shape

"""#### Duplicate Data"""

df_bank.duplicated().sum()

"""#### Outliers Data"""

# Melihat jumlah outlier

Jumlah_Outlier = []

for i in df_bank.columns:

  # Melakukan looping untuk mencari outlier setiap kolom yang isinya bukan tipe data object
  if (df_bank[i].dtypes != 'object'):
    data = df_bank[i]

    # Mencari nilai IQR
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1

    # Mencari RUB dan RLB
    RLB = Q1 - 1.5 * IQR
    RUB = Q3 + 1.5 * IQR

    # Mencari banyak outlier
    outlier_lower = data[data < RLB]
    outlier_upper = data[data > RUB]
    jumlah_outlier = len(outlier_lower) + len(outlier_upper)
    persentase_outlier = jumlah_outlier/len(df_bank)

    Jumlah_Outlier.append([i, jumlah_outlier, persentase_outlier])

pd.DataFrame(Jumlah_Outlier, columns=['Column', 'Jumlah Outliers','Persentase Outliers']).sort_values(by=['Jumlah Outliers'], ascending = False,ignore_index=True)

"""Tergolong normal, tidak perlu dilakukan apapun pada outliers data

### Labeling Data Categorical
"""

def categorical(column):
  le = LabelEncoder()
  df_bank[column] = le.fit_transform(df_bank[column])

for i in df_bank.columns:
  if df_bank[i].dtypes == object:
    categorical(i)

"""## Modeling

### Classification
"""

def fit_and_score(models, X_train, y_train, X_test, y_test):
    model_scores = {}

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        scores = {}
        scores["score"] = model.score(X_test, y_test)
        scores["precision-weighted"] = precision_score(y_test, y_pred, average='weighted')
        scores["recall-weighted"] = recall_score(y_test, y_pred, average='weighted')
        scores["f1-score-weighted"] = f1_score(y_test, y_pred, average='weighted')

        model_scores[name] = scores

    return model_scores

df_classification = df_bank.copy()

df_norm_classification_1 = df_classification.drop(['Attrition_Flag'], axis=1)
df_classification[df_norm_classification_1.columns] = MinMaxScaler().fit_transform(df_norm_classification_1)
df_norm_classification_1 = df_classification.copy()

df_norm_classification_1.tail()

X = df_norm_classification_1.drop(['Attrition_Flag'], axis=1)
y_1 = df_norm_classification_1['Attrition_Flag']

counter_y_1 = Counter(y_1)
print(counter_y_1)

plt.figure(figsize=(20,10))

sns.heatmap(df_norm_classification_1.corr(), vmin=None, vmax=None,
                cmap=sns.diverging_palette(220, 10, as_cmap=True), center=None,
                robust=False, annot=True,
                fmt='.2f', annot_kws=None,
                linewidths=0, linecolor='white',
                cbar_kws=None, cbar_ax=None, square=False,
                xticklabels='auto', yticklabels='auto',
                mask=None, ax=None)

plt.title("Matriks Korelasi Pearson")

X = X.drop(['Avg_Open_To_Buy'], axis=1)

X.shape

"""#### Feature Selection"""

model_scores = {}

def feature_imbalanced(name, model):

  cv_classification = KFold(random_state=2110, shuffle=True)
  max_mean = 0
  max_param = None
  scores_list = []

  X_model, y_model = model.fit_resample(X, y_1)

  print(Counter(y_model))

  for i in range(2, 18):
    score_list = []

    selector = SelectKBest(k=i)
    Xi = selector.fit_transform(X_model, y_model)

    print(selector.get_feature_names_out(input_features=selector.feature_names_in_))

    #Decision Tree
    dtc = DecisionTreeClassifier()
    scores = cross_val_score(dtc, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Decision Tree Classifier:", np.mean(scores))
    score_list.append(np.mean(scores))

    #Random Forest
    rfc = RandomForestClassifier()
    scores = cross_val_score(rfc, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Random Forest Classifier:", np.mean(scores))
    score_list.append(np.mean(scores))

    #Gaussian Naive Bayes
    nb = GaussianNB()
    scores = cross_val_score(nb, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Naive Bayes:", np.mean(scores))
    score_list.append(np.mean(scores))

    #KNN
    knn = KNeighborsClassifier()
    scores = cross_val_score(knn, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score KNN:", np.mean(scores))
    score_list.append(np.mean(scores))

    #Softmax
    sofreg = LogisticRegression(multi_class='multinomial', max_iter=200)
    scores = cross_val_score(sofreg, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Softmax:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #Linear SVC
    lin_svc = LinearSVC()
    scores = cross_val_score(lin_svc, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Linear SVC:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #SVC
    svc = SVC()
    scores = cross_val_score(svc, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score SVC :", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #Percepton
    percepton = Perceptron()
    scores = cross_val_score(percepton, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Percepton:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #SGD
    sgd = SGDClassifier()
    scores = cross_val_score(sgd, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score SGD:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #XGB
    xgb = XGBClassifier()
    scores = cross_val_score(xgb, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score XGB:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #Adaboost
    adaboost = AdaBoostClassifier()
    scores = cross_val_score(adaboost, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score Adaboost:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    #GradientBoostingClassifier
    grad_boost = GradientBoostingClassifier()
    scores = cross_val_score(grad_boost, Xi, y_model, scoring='f1_weighted', cv=cv_classification)
    print("Score GradientBoostingClassifier:", np.mean(scores))
    score_list.append(np.mean(scores))
    scores_list.append(score_list)

    if np.mean(score_list) > max_mean:
      max_mean = np.mean(score_list)
      max_param = selector.get_feature_names_out(input_features=selector.feature_names_in_)
    print()

  scores = {}
  scores["max_param"] = max_param

  model_scores[name] = scores

feature_imbalanced("SMOTE", SMOTE())

feature_imbalanced("Random Oversampling", RandomOverSampler(sampling_strategy='minority'))

feature_imbalanced("Borderline-SMOTE", BorderlineSMOTE())

feature_imbalanced("Borderline Oversampling with SVM", SVMSMOTE())

feature_imbalanced("Random Undersampling", RandomUnderSampler(sampling_strategy='majority'))

feature_imbalanced("NearMiss", NearMiss())

data_name = []
data_params = []
for key, value in model_scores.items():
  data_name.append(key)
  data_params.append(list(value.values())[0])

feature_imbalanced = pd.DataFrame({
    'teknik': data_name,
    'max_param': data_params
})
feature_imbalanced

"""#### Sampling

##### Random Oversampling
"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'Random Oversampling'].values[0]
max_param

X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

model = RandomOverSampler(sampling_strategy='minority')

X_model, y_model = model.fit_resample(X_train_1, y_train_1)

Counter(y_model)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gaussian": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Softmax Regression": LogisticRegression(multi_class='multinomial', max_iter=35000),
    "LinearSVC": LinearSVC(),
    "SVC": SVC(),
    "Percepton": Perceptron(),
    "SGD": SGDClassifier(),
    "XGB": XGBClassifier(),
    'Adaboost': AdaBoostClassifier(),
    'GradientBoostingClassifier': GradientBoostingClassifier()
}

fit_and_score_results_1 = fit_and_score(models, X_model, y_model, X_test_1, y_test_1)

scores_df_1 = pd.DataFrame(fit_and_score_results_1).transpose()
for c in scores_df_1.columns:
    if c == 'recall':
        continue
    scores_df_1[c] = scores_df_1[c].astype(float)
scores_df_1

scores_df_1.reset_index(inplace=True)

scores_df_1 = scores_df_1.sort_values('score')
plt.bar(scores_df_1['index'], scores_df_1['score'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('precision-weighted')
plt.bar(scores_df_1['index'], scores_df_1['precision-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('recall-weighted')
plt.bar(scores_df_1['index'], scores_df_1['recall-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('f1-score-weighted')
plt.bar(scores_df_1['index'], scores_df_1['f1-score-weighted'])
plt.xticks(rotation='vertical')
plt.show()

"""###### XGB"""

cv_classification = KFold(random_state=2110, shuffle=True)
xgb = XGBClassifier()
scores = cross_val_score(xgb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score XGB:", scores.mean())
print("Standar Deviasi Score XGB:", scores.std())

"""###### Random Forest"""

cv_classification = KFold(random_state=2110, shuffle=True)
rf = RandomForestClassifier()
scores = cross_val_score(rf, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score Random Forest:", scores.mean())
print("Standar Deviasi Score Random Forest:", scores.std())

"""##### SMOTE"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'SMOTE'].values[0]
max_param

X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

model = SMOTE()

X_model, y_model = model.fit_resample(X_train_1, y_train_1)

Counter(y_model)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gaussian": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Softmax Regression": LogisticRegression(multi_class='multinomial', max_iter=35000),
    "LinearSVC": LinearSVC(),
    "SVC": SVC(),
    "Percepton": Perceptron(),
    "SGD": SGDClassifier(),
    "XGB": XGBClassifier(),
    'Adaboost': AdaBoostClassifier(),
    'GradientBoostingClassifier': GradientBoostingClassifier()
}

fit_and_score_results_1 = fit_and_score(models, X_model, y_model, X_test_1, y_test_1)

scores_df_1 = pd.DataFrame(fit_and_score_results_1).transpose()
for c in scores_df_1.columns:
    if c == 'recall':
        continue
    scores_df_1[c] = scores_df_1[c].astype(float)
scores_df_1

scores_df_1.reset_index(inplace=True)

scores_df_1 = scores_df_1.sort_values('score')
plt.bar(scores_df_1['index'], scores_df_1['score'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('precision-weighted')
plt.bar(scores_df_1['index'], scores_df_1['precision-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('recall-weighted')
plt.bar(scores_df_1['index'], scores_df_1['recall-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('f1-score-weighted')
plt.bar(scores_df_1['index'], scores_df_1['f1-score-weighted'])
plt.xticks(rotation='vertical')
plt.show()

"""###### XGB"""

cv_classification = KFold(random_state=2110, shuffle=True)
xgb = XGBClassifier()
scores = cross_val_score(xgb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score XGB:", scores.mean())
print("Standar Deviasi Score XGB:", scores.std())

"""###### GradientBoostingClassifier"""

cv_classification = KFold(random_state=2110, shuffle=True)
gb = GradientBoostingClassifier()
scores = cross_val_score(gb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score GradientBoostingClassifier:", scores.mean())
print("Standar Deviasi Score GradientBoostingClassifier:", scores.std())

"""##### Borderline Oversampling with SVM"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'Borderline Oversampling with SVM'].values[0]
max_param

X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

model = SVMSMOTE()

X_model, y_model = model.fit_resample(X_train_1, y_train_1)

Counter(y_model)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gaussian": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Softmax Regression": LogisticRegression(multi_class='multinomial', max_iter=35000),
    "LinearSVC": LinearSVC(),
    "SVC": SVC(),
    "Percepton": Perceptron(),
    "SGD": SGDClassifier(),
    "XGB": XGBClassifier(),
    'Adaboost': AdaBoostClassifier(),
    'GradientBoostingClassifier': GradientBoostingClassifier()
}

fit_and_score_results_1 = fit_and_score(models, X_model, y_model, X_test_1, y_test_1)

scores_df_1 = pd.DataFrame(fit_and_score_results_1).transpose()
for c in scores_df_1.columns:
    if c == 'recall':
        continue
    scores_df_1[c] = scores_df_1[c].astype(float)
scores_df_1

scores_df_1.reset_index(inplace=True)

scores_df_1 = scores_df_1.sort_values('score')
plt.bar(scores_df_1['index'], scores_df_1['score'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('precision-weighted')
plt.bar(scores_df_1['index'], scores_df_1['precision-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('recall-weighted')
plt.bar(scores_df_1['index'], scores_df_1['recall-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('f1-score-weighted')
plt.bar(scores_df_1['index'], scores_df_1['f1-score-weighted'])
plt.xticks(rotation='vertical')
plt.show()

"""###### XGB"""

cv_classification = KFold(random_state=2110, shuffle=True)
xgb = XGBClassifier()
scores = cross_val_score(xgb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score XGB:", scores.mean())
print("Standar Deviasi Score XGB:", scores.std())

"""###### GradientBoostingClassifier"""

cv_classification = KFold(random_state=2110, shuffle=True)
gb = GradientBoostingClassifier()
scores = cross_val_score(gb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score GradientBoostingClassifier:", scores.mean())
print("Standar Deviasi Score GradientBoostingClassifier:", scores.std())

"""##### Borderline-SMOTE"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'Borderline-SMOTE'].values[0]
max_param

X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

model = BorderlineSMOTE()

X_model, y_model = model.fit_resample(X_train_1, y_train_1)

Counter(y_model)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gaussian": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Softmax Regression": LogisticRegression(multi_class='multinomial', max_iter=35000),
    "LinearSVC": LinearSVC(),
    "SVC": SVC(),
    "Percepton": Perceptron(),
    "SGD": SGDClassifier(),
    "XGB": XGBClassifier(),
    'Adaboost': AdaBoostClassifier(),
    'GradientBoostingClassifier': GradientBoostingClassifier()
}

fit_and_score_results_1 = fit_and_score(models, X_model, y_model, X_test_1, y_test_1)

scores_df_1 = pd.DataFrame(fit_and_score_results_1).transpose()
for c in scores_df_1.columns:
    if c == 'recall':
        continue
    scores_df_1[c] = scores_df_1[c].astype(float)
scores_df_1

scores_df_1.reset_index(inplace=True)

scores_df_1 = scores_df_1.sort_values('score')
plt.bar(scores_df_1['index'], scores_df_1['score'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('precision-weighted')
plt.bar(scores_df_1['index'], scores_df_1['precision-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('recall-weighted')
plt.bar(scores_df_1['index'], scores_df_1['recall-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('f1-score-weighted')
plt.bar(scores_df_1['index'], scores_df_1['f1-score-weighted'])
plt.xticks(rotation='vertical')
plt.show()

"""###### XGB"""

cv_classification = KFold(random_state=2110, shuffle=True)
xgb = XGBClassifier()
scores = cross_val_score(xgb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score XGB:", scores.mean())
print("Standar Deviasi Score XGB:", scores.std())

"""###### Random Forest"""

cv_classification = KFold(random_state=2110, shuffle=True)
rf = RandomForestClassifier()
scores = cross_val_score(rf, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score Random Forest:", scores.mean())
print("Standar Deviasi Score Random Forest:", scores.std())

"""##### Random Undersampling"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'Random Undersampling'].values[0]
max_param

X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

model = NearMiss()

X_model, y_model = model.fit_resample(X_train_1, y_train_1)

Counter(y_model)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gaussian": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Softmax Regression": LogisticRegression(multi_class='multinomial', max_iter=35000),
    "LinearSVC": LinearSVC(),
    "SVC": SVC(),
    "Percepton": Perceptron(),
    "SGD": SGDClassifier(),
    "XGB": XGBClassifier(),
    'Adaboost': AdaBoostClassifier(),
    'GradientBoostingClassifier': GradientBoostingClassifier()
}

fit_and_score_results_1 = fit_and_score(models, X_model, y_model, X_test_1, y_test_1)

scores_df_1 = pd.DataFrame(fit_and_score_results_1).transpose()
for c in scores_df_1.columns:
    if c == 'recall':
        continue
    scores_df_1[c] = scores_df_1[c].astype(float)
scores_df_1

scores_df_1.reset_index(inplace=True)

scores_df_1 = scores_df_1.sort_values('score')
plt.bar(scores_df_1['index'], scores_df_1['score'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('precision-weighted')
plt.bar(scores_df_1['index'], scores_df_1['precision-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('recall-weighted')
plt.bar(scores_df_1['index'], scores_df_1['recall-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('f1-score-weighted')
plt.bar(scores_df_1['index'], scores_df_1['f1-score-weighted'])
plt.xticks(rotation='vertical')
plt.show()

"""###### GradientBoostingClassifier"""

cv_classification = KFold(random_state=2110, shuffle=True)
gb = GradientBoostingClassifier()
scores = cross_val_score(gb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score GradientBoostingClassifier:", scores.mean())
print("Standar Deviasi Score GradientBoostingClassifier:", scores.std())

"""###### Adaboost"""

cv_classification = KFold(random_state=2110, shuffle=True)
ada = AdaBoostClassifier()
scores = cross_val_score(ada, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score Adaboost:", scores.mean())
print("Standar Deviasi Score Adaboost:", scores.std())

"""##### NearMiss"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'NearMiss'].values[0]
max_param

X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

model = NearMiss()

X_model, y_model = model.fit_resample(X_train_1, y_train_1)

Counter(y_model)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gaussian": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Softmax Regression": LogisticRegression(multi_class='multinomial', max_iter=35000),
    "LinearSVC": LinearSVC(),
    "SVC": SVC(),
    "Percepton": Perceptron(),
    "SGD": SGDClassifier(),
    "XGB": XGBClassifier(),
    'Adaboost': AdaBoostClassifier(),
    'GradientBoostingClassifier': GradientBoostingClassifier()
}

fit_and_score_results_1 = fit_and_score(models, X_model, y_model, X_test_1, y_test_1)

scores_df_1 = pd.DataFrame(fit_and_score_results_1).transpose()
for c in scores_df_1.columns:
    if c == 'recall':
        continue
    scores_df_1[c] = scores_df_1[c].astype(float)
scores_df_1

scores_df_1.reset_index(inplace=True)

scores_df_1 = scores_df_1.sort_values('score')
plt.bar(scores_df_1['index'], scores_df_1['score'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('precision-weighted')
plt.bar(scores_df_1['index'], scores_df_1['precision-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('recall-weighted')
plt.bar(scores_df_1['index'], scores_df_1['recall-weighted'])
plt.xticks(rotation='vertical')
plt.show()

scores_df_1 = scores_df_1.sort_values('f1-score-weighted')
plt.bar(scores_df_1['index'], scores_df_1['f1-score-weighted'])
plt.xticks(rotation='vertical')
plt.show()

"""###### XGB"""

cv_classification = KFold(random_state=2110, shuffle=True)
xgb = XGBClassifier()
scores = cross_val_score(xgb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score XGB:", scores.mean())
print("Standar Deviasi Score XGB:", scores.std())

"""###### GradientBoostingClassifier"""

cv_classification = KFold(random_state=2110, shuffle=True)
gb = GradientBoostingClassifier()
scores = cross_val_score(gb, X_model, y_model, scoring='f1_weighted', cv=cv_classification)
print("Rata-Rata Score GradientBoostingClassifier:", scores.mean())
print("Standar Deviasi Score GradientBoostingClassifier:", scores.std())

"""#### Pilih Model dan tuning hyperparameter"""

from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, \
    recall_score, classification_report, \
    accuracy_score, f1_score
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

def evaluate_classifier_performance(prediction, y_test):
    print("Hasil Evaluasi berdasarkan classification report \n\n%s\n" % (classification_report(y_test, prediction,zero_division=0)))
    print()
    print("Confusion Matrix")
    print()
    y_actual = pd.Series(np.array(y_test), name = "actual")
    y_pred = pd.Series(np.array(prediction), name = "prediction")
    df_confusion = pd.crosstab(y_actual, y_pred)
    display(df_confusion)
    print()
    print()

    print("Butuh informasi lebih lengkap? silakan simak di bawah ini : ")
    print('Accuracy Average:', accuracy_score(y_test, prediction))
    print('F1 Macro Average:', f1_score(y_test, prediction, average='macro'))
    print('F1 Micro Average:', f1_score(y_test, prediction, average='micro'))
    print('Precision Macro Average:', precision_score(y_test, prediction, average='macro',zero_division=0))
    print('Precision Micro Average:', precision_score(y_test, prediction, average='micro',zero_division=0))
    print('Recall Macro Average:', recall_score(y_test, prediction, average='macro',zero_division=0))
    print('Recall Micro Average:', recall_score(y_test, prediction, average='micro',zero_division=0))
    print()

from sklearn.model_selection import cross_val_score, KFold

def train_and_evaluate_model(pipeline, param_grid, model_name, X_train, y_train, X_test, y_test):
    grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_weighted', cv=5)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    print(best_model)

    y_pred = best_model.predict(X_test)

    evaluate_classifier_performance(y_pred, y_test)

"""##### XGBClassifier dengan Random Oversampling"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'Random Oversampling'].values[0]
X_1 = X[max_param]

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

pipeline = Pipeline([
    ('oversample', RandomOverSampler(sampling_strategy='minority')),
    ('classifier', XGBClassifier())
])

param_grid = {
    'classifier__min_child_weight': [1, 5, 10],
    'classifier__gamma': [0.5, 1, 1.5, 2, 5],
    'classifier__colsample_bytree': [0.6, 0.8, 1.0],
    'classifier__max_depth': [3, 4, 5]
}

train_and_evaluate_model(pipeline, param_grid, "XGBClassifier dengan Random Oversampling", X_train_1, y_train_1, X_test_1, y_test_1)

"""##### Random Forest dengan Borderline-Random Oversampling"""

max_param = feature_imbalanced['max_param'].loc[feature_imbalanced['teknik'] == 'Random Oversampling'].values[0]
X_1 = X[max_param]

X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_1, y_1, stratify=y_1, train_size=0.8, random_state=2110)

pipeline = Pipeline([
    ('oversample', RandomOverSampler(sampling_strategy='minority')),
    ('classifier', RandomForestClassifier())
])

param_grid = {
    'classifier__n_estimators': [25, 50, 100, 150],
    'classifier__max_features': ['sqrt', 'log2', None],
    'classifier__max_depth': [3, 6, 9],
    'classifier__max_leaf_nodes': [3, 6, 9],
}

train_and_evaluate_model(pipeline, param_grid, "Random Forest dengan Borderline-Random Oversampling", X_train_2, y_train_2, X_test_2, y_test_2)

"""#### Kesimpulan Modeling Classification

**XGBClassifier dengan Random Oversampling**

Fitur yang digunakan:
'Gender', 'Dependent_count', 'Education_Level', 'Marital_Status', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',   'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio'

## Evaluasi

**XGBClassifier dengan Random Oversampling**

Fitur yang digunakan: 'Gender', 'Dependent_count', 'Education_Level', 'Marital_Status', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio'

Hasil Evaluasi berdasarkan classification report

              precision    recall  f1-score   support
           0       0.87      0.92      0.90       223
           1       0.98      0.97      0.98      1194

    accuracy                           0.97      1417
    macro avg      0.93      0.95      0.94      1417
    weighted avg   0.97      0.97      0.97      1417

Confusion Matrix

    prediction/actual        0        1
           0                 205      18
           1                 30       1164


*   Accuracy Average: 0.9661256175017643
*   F1 Macro Average: 0.937497243174099
*   F1 Micro Average: 0.9661256175017643
*   Precision Macro Average: 0.928555999567988
*   Precision Micro Average: 0.9661256175017643
*   Recall Macro Average: 0.9470784415350294
*   Recall Micro Average: 0.9661256175017643
"""